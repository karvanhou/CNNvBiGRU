{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM9HB-Hilbjo"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Bidirectional, GRU, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime"
      ],
      "metadata": {
        "id": "QhXD3DOVTxHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import lime\n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer"
      ],
      "metadata": {
        "id": "a56PzOawT2l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgS9v8ZWli47"
      },
      "outputs": [],
      "source": [
        "#Get access to the dataset which is uploaded on my google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DxA-0Aflo3T"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive//MyDrive/ColabNotebooks/Android_Opcode_Sequences.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "#The opcodes are space-separated in the CSV file and Split them into lists.\n",
        "#Convert opcode sequences, which are represented as space-separated strings in CSV file, into lists of individual opcodes.\n",
        "data['opcodes'] = data['opcodes'].apply(lambda x: x.split())\n",
        "\n",
        "# Separate features and labels\n",
        "opcode_sequences = data['opcodes'].tolist()\n",
        "y = data['labels'].values\n",
        "\n",
        "# I have trained a Word2Vec model on the opcode sequences.\n",
        "# Tokenize opcode sequences to list of words (opcodes)\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=opcode_sequences, vector_size=10, window=5, min_count=1, workers=4,sg=0)\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "# Convert opcode sequences to vectors\n",
        "vectorized_sequences = [[word2vec_model.wv[word] for word in sequence] for sequence in opcode_sequences]\n",
        "# Find the maximum sequence length\n",
        "max_length = max(len(sequence) for sequence in vectorized_sequences)\n",
        "# Pad sequences\n",
        "X = pad_sequences(vectorized_sequences, maxlen=max_length, dtype='float32', padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhXfTNZoCSyL"
      },
      "outputs": [],
      "source": [
        "#Visualize the word vectors using PCA technique to see if similar words are clustered together in the vector space.\n",
        "\n",
        "words = list(word2vec_model.wv.index_to_key)\n",
        "vectors = [word2vec_model.wv[word] for word in words]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(vectors)\n",
        "\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rtBGukoKifp"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Check the shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential([\n",
        "    # Convolutional Layer\n",
        "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(max_length, 10)),# 64 is the vector_size in Word2Vec model\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Bi-directional GRU Layers\n",
        "    Bidirectional(GRU(64, return_sequences=True)),\n",
        "    Bidirectional(GRU(32)),\n",
        "\n",
        "    # Flatten Layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully Connected Neural Network Module\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),  # Dropout Regularization\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),  # Dropout Regularization\n",
        "\n",
        "    # Output Layer\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid Activation for Binary Classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "#Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "fVv1iIUWKd76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "_, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "test_predictions = (model.predict(X_test)> 0.5).astype(int)\n",
        "test_predictions.shape\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "true_negative, false_positive, false_negative, true_positive = conf_matrix.ravel()\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Print individual components of the confusion matrix\n",
        "#how many samples were classified as benign software and was.\n",
        "print(\"True Negative (TN):\", true_negative)\n",
        "#how many samples were classified as malware but were not.\n",
        "print(\"False Positive (FP):\", false_positive)\n",
        "#how many samples were classified as benign software but was not.\n",
        "print(\"False Negative (FN):\", false_negative)\n",
        "#how many samples were classified as malware and was.\n",
        "print(\"True Positive (TP):\", true_positive)"
      ],
      "metadata": {
        "id": "hh2XdwwrUT11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjP6MfpPTHW3"
      },
      "outputs": [],
      "source": [
        "for i in range(1486):\n",
        "    if (test_predictions[i]) != (y_test[i]):\n",
        "      print('%d (expected %d)' % ((test_predictions[i]),(y_test[i])))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a LimeTextExplainer\n",
        "explainer = lime_text.LimeTextExplainer(class_names=['Benign(0)', 'Malware(1)'])\n",
        "\n",
        "#  idx is the index of the instance in the dataset you want to explain\n",
        "idx = 100 # the index of the instance you want to explain\n",
        "instance = ' '.join(opcode_sequences[idx])  # Converting list of opcodes to space-separated string\n",
        "# Define a prediction function\n",
        "def predictor(texts):\n",
        "    # Initialize list to hold sequences\n",
        "    sequences = []\n",
        "\n",
        "    # Convert texts to sequences of word vectors\n",
        "    for text in texts:\n",
        "        sequence = []\n",
        "        words = text.split()  # Split text into words\n",
        "\n",
        "        for word in words:\n",
        "            # Check if the word is in the model's vocabulary\n",
        "            if word in word2vec_model.wv.index_to_key:\n",
        "                # Append the word vector to the sequence\n",
        "                sequence.append(word2vec_model.wv[word])\n",
        "\n",
        "        # Append the sequence to the list of sequences\n",
        "        sequences.append(sequence)\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, dtype='float32', padding='post')\n",
        "\n",
        "    # Get predictions from the model\n",
        "    predictions = model.predict(padded_sequences)\n",
        "\n",
        "    # Return stacked predictions\n",
        "    return np.hstack((1 - predictions, predictions))\n",
        "exp = explainer.explain_instance(instance, predictor, num_features=8)\n",
        "\n",
        "\n",
        "# Show explanation in the notebook\n",
        "exp.show_in_notebook(text=True)\n"
      ],
      "metadata": {
        "id": "QnllXK5dhk9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpmDgkXJdn81"
      },
      "outputs": [],
      "source": [
        "#following snippet plots the graph of training accuracy vs. validation accuracy over the number of epochs.\n",
        "# Assuming history is the return value from model.fit(...)\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Subplot for accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}